{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/budatijyothi12/AI-Agent/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TgJKIjf06tD",
        "outputId": "e9063b8c-71a4-44dc-e701-7c1ee400217b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lowercasing"
      ],
      "metadata": {
        "id": "CNICUtRc1N3d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BORUdOmZzkcd",
        "outputId": "e3c3e5c1-b1aa-485e-c2b0-971fae771c18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this product is amazing!\n"
          ]
        }
      ],
      "source": [
        "text = \"This Product is AMAZING!\"\n",
        "text_lower = text.lower()\n",
        "print(text_lower)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Punctuation"
      ],
      "metadata": {
        "id": "bfKu7jW_1UfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Wow!!! This product is super cool, right?\"\n",
        "text_no_punct = re.sub(r'[^\\w\\s]', '', text)\n",
        "print(text_no_punct)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kypAw_QXzq3U",
        "outputId": "2f44d27f-82a7-4837-bd56-2942fef560e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wow This product is super cool right\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Remove Stopwords"
      ],
      "metadata": {
        "id": "Kf5ccaqT1aZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "text = \"This product is really very good and useful\"\n",
        "words = text.lower().split()\n",
        "filtered = [word for word in words if word not in stop_words]\n",
        "print(filtered)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjk7Q00Czq5w",
        "outputId": "80bdb58b-4069-4dcc-aa06-fdcb40af111b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['product', 'really', 'good', 'useful']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üí• 4. Remove Numbers"
      ],
      "metadata": {
        "id": "nnIkt0JM1cjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I bought 2 items and got 50% off\"\n",
        "text_no_numbers = re.sub(r'\\d+', '', text)\n",
        "print(text_no_numbers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXouwGECzwES",
        "outputId": "0b624d6b-f632-4a4c-b761-ab22f1ad09b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I bought  items and got % off\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåê 5. Remove URLs\n"
      ],
      "metadata": {
        "id": "rhVckZNB1ium"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Check this out: https://coolstuff.com/deals\"\n",
        "text_no_url = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "print(text_no_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUZ7TU74zwG5",
        "outputId": "076de7b4-0399-4838-b9dd-37ae43ac2c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check this out: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ 6. Tokenization"
      ],
      "metadata": {
        "id": "o2BTMavz1pQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing is fun\"\n",
        "tokens = text.split()\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci2ZaYGJz1Wb",
        "outputId": "5c6fc0ee-5af7-47d2-b64b-a0d686ac0dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'fun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üå± 7. Stemming"
      ],
      "metadata": {
        "id": "ZAjirN3n1rd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"playing\", \"played\", \"plays\", \"player\"]\n",
        "stemmed = [stemmer.stem(word) for word in words]\n",
        "print(stemmed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5wLc61hz1YR",
        "outputId": "d323df90-52c3-48c8-ffc2-9387245b31b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['play', 'play', 'play', 'player']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåø 8. Lemmatization\n"
      ],
      "metadata": {
        "id": "MhdDASEJ1w6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"ran\", \"runs\"]\n",
        "lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "print(lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfzr-jFYz1bp",
        "outputId": "b2aa6881-4b69-4212-8fb9-da7c091afa60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'run', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîπ 1. ü•≤ Emoji Removal"
      ],
      "metadata": {
        "id": "DoC16vJK16S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"I love this product üòçüî•üíØ!\"\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "text_no_emoji = emoji_pattern.sub(r'', text)\n",
        "print(text_no_emoji)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxy92v45zwKe",
        "outputId": "f631885d-7a79-40e0-c219-9165f204422b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love this product !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîπ 2. üß† TF-IDF (Term Frequency - Inverse Document Frequency)\n"
      ],
      "metadata": {
        "id": "Agwu6j5818Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is amazing\",\n",
        "    \"I love deep learning and AI\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "8xao0HvZzq9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(docs)\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIXH8_BV0LBJ",
        "outputId": "cb28ca97-470b-4ab7-e0ab-bc77c843c377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "{'love': 6, 'machine': 7, 'learning': 5, 'is': 4, 'amazing': 1, 'deep': 3, 'and': 2, 'ai': 0}\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.         0.         0.48133417\n",
            "  0.61980538 0.61980538]\n",
            " [0.         0.5844829  0.         0.         0.5844829  0.34520502\n",
            "  0.         0.44451431]\n",
            " [0.50461134 0.         0.50461134 0.50461134 0.         0.29803159\n",
            "  0.38376993 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Handling Contractions\n"
      ],
      "metadata": {
        "id": "ODpLvK312K1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "text = \"I don't like this\"\n",
        "expanded_text = contractions.fix(text)\n",
        "print(expanded_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HemRcDpQ0LDG",
        "outputId": "bbb0e4b3-7d90-468b-f257-0ac0fc811885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I do not like this\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Handling Misspellings\n"
      ],
      "metadata": {
        "id": "TaIjmKNZ2bN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I reaaaly lovd thish produckt! soo mooch \"\n",
        "corrected_text = str(TextBlob(text).correct())\n",
        "print(corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezMJZ9hh2b-i",
        "outputId": "52b4f006-83f8-468c-edf3-2ae150701d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I really love this product! so much \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Removing Extra Whitespaces\n"
      ],
      "metadata": {
        "id": "QlCFFo0l2QDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This   is   spaced   out\"\n",
        "text = re.sub('\\s+', ' ', text).strip()\n"
      ],
      "metadata": {
        "id": "BQCmml_n0LGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}